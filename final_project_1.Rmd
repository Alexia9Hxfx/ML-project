---
title: "project"
output: pdf_document
date: "2023-04-25"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
options(warn=-1)
```


##1.1 Data Preparation
###1.1 read data and environment
```{r}

library(readr)
library(dplyr)
library(tidyverse)
library(randomForest)
library(caTools)
library(rsample)
library(corrplot)
library(ggplot2)
library(plotmo)
library(glmnet)
library(pROC)
library(ggthemes)
library(tidyverse) 
library(caret)
library(scales)
library(rcompanion)
library(patchwork)
library(forcats)
library(ggthemes)
library(gbm)
options(warn=-1)
warning=FALSE
library(dplyr, warn.conflicts = FALSE)
options(dplyr.summarise.inform = FALSE)
data <- read_csv("~/Desktop/KAG_conversion_data.csv")
head(data,10)
```

###1.2 data clean, factor

```{r}
# data clean
# Plot the distribution of 'Approved_conversion'
ggplot(data, aes(x = Approved_Conversion)) +
  geom_histogram() +
  labs(title = "Distribution of Approved_conversion", x = "Approved_conversion", y = "Frequency")

#change conversion into dummy
data_clean <- data %>% 
  mutate(xyz_campaign_id = as.factor(xyz_campaign_id),
         age = as.factor(age),
         gender = as.factor(gender),
         interest = as.factor(interest),
         target = as.factor(ifelse(Approved_Conversion==0,0,1))
  )
data_clean<-na.omit(data_clean)
data_clean <- subset(data_clean, select= -c(fb_campaign_id,Approved_Conversion))
```
###1.3 Correlation plot
```{r}
ind_factor <- sapply(data_clean, is.factor)
data_clean_cor <- data_clean
data_clean_cor[ind_factor] <- lapply(data_clean_cor[ind_factor], function(x) as.numeric(x))

data_clean_cor %>%
  dplyr::select(2:9) %>% 
  scale() %>%
  cor() %>%
  corrplot(method = 'color', 
                     type = 'upper', 
                     diag = F, 
                     tl.col = 'black',
                     addCoef.col = "white",
                     number.cex = 0.7,
                     tl.cex = 0.7,
                     main = 'Correlation of the Adv. variables',
                     mar = c(1,0,2,0))
```
###1.4 density plots
```{r}
#continuous variables density plot
data_clean[,-1] %>% select_if(is.numeric) %>% 
  gather() %>% 
  ggplot(aes(value)) +
  facet_wrap(~ key, scales = "free") + geom_density()+
  ggtitle("continuous variable density plot") +
  theme(plot.title = element_text(hjust = 0.5))

#categorical variables density plot
data_clean %>% select_if(is.factor) %>% 
  gather() %>% 
  ggplot(aes(value)) + facet_wrap(~ key, scales = "free") + geom_histogram(stat = 'count')+
  ggtitle("categorical variable density plot") +
  theme(plot.title = element_text(hjust = 0.5))
```
Based on the domain knowledge about ads campagin, we observed that age, spent, and interst are the most important factors which would determine whether the ad would do the conversion. 
```{r}
# First we use age's and spent's influence on target
ggplot(data_clean, aes(x = Spent, y = target, fill = age)) +
  geom_boxplot() +
  labs(x = "Spent", y = "target", fill = "Age",title = "Effect of Spent and Age on Target")+
  theme(plot.title = element_text(size = 20, face = "bold"),
        axis.title = element_text(size = 16),
        axis.text = element_text(size = 14),
        legend.title = element_text(size = 16),
        legend.text = element_text(size = 14))
# Then we use age's, spent's and interest's influence on target
ggplot(data_clean, aes(x = Spent, y = target, fill = as.factor(interest))) +
  geom_boxplot() +
  labs(x = "Spent", y = "Target", fill = "Interest", title = "Effect of Spent, Age and Interest on Target") +
  scale_fill_manual(values = rainbow(40)) +
  theme(plot.title = element_text(size = 20, face = "bold"),
        axis.title = element_text(size = 16),
        axis.text = element_text(size = 14),
        legend.title = element_text(size = 16),
        legend.text = element_text(size = 14))

# Fit a logistic regression model
model <- glm(target ~ ., data = data_clean, family = binomial)

# Extract the coefficient estimates and sort them by absolute value
coef_df <- data.frame(variable = names(coef(model))[-1], # exclude the intercept
                      coefficient = abs(coef(model)[-1]))
coef_df <- coef_df[order(-coef_df$coefficient), ]

# Select the top 10 variables with the largest absolute coefficient values
top10_vars <- coef_df$variable[1:10]
top10_vars



```
We find that top 10 variables are all "interest". Thus, we don't think we need the interation term to add in the classification model. Also, 

```{r}
#summary of each company
camp_summary <- data_clean %>%
  group_by(xyz_campaign_id) %>%
  summarise(Impressions = sum(Impressions),
            Clicks = sum(Clicks),
            Spent = sum(Spent),
            Total_Conversion = sum(Total_Conversion),
            target = sum(as.numeric(target))) %>%
  ungroup() %>%
  mutate(`Spent/Imp(k)` = Spent/Impressions*1000,
         `Conversion/Imp(k)` = Total_Conversion/Impressions*1000)
camp_summary
# Convert xyz_campaign_id to factor variable with defined levels
data_clean$xyz_campaign_id <- factor(data_clean$xyz_campaign_id, levels = c(916, 936, 1178))
# Create contingency table
cont_table <- table(data_clean$xyz_campaign_id, data_clean$target)

# View contingency table
cont_table

# Perform pairwise comparisons of proportions
pairwise.prop.test(cont_table, p.adjust.method = "bonferroni")
```

We found that campaign id 1178 has the most Impressions, conversion in total. Besides, the results of pairwise comparisons of proportions between the three ad campaign IDs using the pairwise.prop.test() function with Bonferroni adjustment.

The output indicates that there is a significant difference in the proportion of converted users between ad campaign IDs 916 and 1178, with a Bonferroni-adjusted p-value of 0.049. This means that the proportion of converted users is significantly higher for ad campaign ID 1178 than for ad campaign ID 916.

There is also a significant difference in the proportion of converted users between ad campaign IDs 936 and 1178, with a Bonferroni-adjusted p-value of 1.6e-15 (essentially zero). This means that the proportion of converted users is significantly higher for ad campaign ID 1178 than for ad campaign ID 936.

We will conduct prediction towards campaign id 1178 alone, since difference campaign have different effect on conversion. 



##2.1 logistic regression toward data_1178
```{r}
#split data
set.seed(12)
data_1178_all_lr <- data_clean %>% filter(xyz_campaign_id==1178)
data_1178_lg <- subset(data_1178_all_lr, select = -c(ad_id,xyz_campaign_id))
ind1 <- 1:(nrow(data_1178_lg)/4)
train_data_lg <- data_1178_lg[-ind1, ]
test_data_lg <- data_1178_lg[ind1, ]
# Perform logistic regression with Lasso regularization on the training set
# Convert categorical variables to numeric using one-hot encoding
train_data_lg <- model.matrix( ~ . -1, data = train_data_lg)
# Convert the data to matrix type
x_train_lasso <- as.matrix(train_data_lg[,-49])
y_train_lasso <- train_data_lg[ ,49]
# Fit the Lasso model
fit_lasso <- cv.glmnet(x_train_lasso, y_train_lasso, family = "binomial", alpha = 1, nfolds = 10)
# Print the cross-validation results to choose the optimal lambda value
plot(fit_lasso)
best_lambda <- fit_lasso$lambda.min
cat("Best lambda value:", best_lambda, "\n")
# Refit the model using the optimal lambda value
fit_lg_best <- glmnet(x_train_lasso, y_train_lasso, family = "binomial", alpha = 1, lambda = best_lambda)
# Make predictions on the test set
# Convert categorical variables to numeric using one-hot encoding
test_data_lg <- model.matrix( ~ . -1, data = test_data_lg)
# Convert the data to matrix type
y_test_lasso <- test_data_lg[,49]
x_test_lasso <- as.matrix(test_data_lg[,-49])
# Make predictions on the test set using the fitted Lasso model
predict_test_lasso <- predict(fit_lg_best, newx = x_test_lasso, type = "response")

# Convert the predictions to binary (0/1) using a threshold of 0.5
pred_test_lasso <- ifelse(predict_test_lasso > 0.5, 1, 0)

# Evaluate the model performance on the test set
# Calculate the Area Under the Receiver Operating Characteristic Curve (AUC-ROC) 
pred_test_lasso <- as.numeric(pred_test_lasso)

roc_data_lg <- roc(y_test_lasso, pred_test_lasso)
auc_lg <- roc(y_test_lasso, predict_test_lasso)$auc
# Create AUC plot
roc_plot_lg <- ggroc(roc_data_lg, legacy.axes = TRUE)
# add the AUC value to the plot
auc_label_lg <- paste("AUC = ", auc_lg)
roc_plot_lg <- roc_plot_lg + labs(title = "ROC Curve", subtitle = auc_label_lg)
roc_plot_lg <- roc_plot_lg + geom_area(aes(x = 1 - specificity, y = sensitivity),
                                 fill = "blue", alpha = 0.2)
roc_plot_lg
# Calculate the confusion matrix and the classification report
confusion_matrix <- table(Actual = y_test_lasso, Predicted = pred_test_lasso)
cat("Confusion Matrix:\n")
print(confusion_matrix)
cat("\n")
classification_report <- caret::confusionMatrix(factor(pred_test_lasso), factor(y_test_lasso))
cat("Classification Report:\n")
print(classification_report)

```


##3.1 KNN

```{r}
set.seed(12)
# Splitting data into train and test data
library(class)
data_clean_knn <- data %>% 
  mutate(age = as.factor(age),
         gender = as.factor(gender),
         Spent = scale(Spent),
         Impressions = scale(Impressions),
         Clicks = scale(Clicks),
         interest = as.factor(interest),
         target = ifelse(Approved_Conversion==0,0,1)
  )
data_clean_knn <- subset(data_clean_knn, select= -c(fb_campaign_id,Approved_Conversion))
data_1178_all_knn <- data_clean_knn %>% filter(xyz_campaign_id==1178)
data_1178_knn <- subset(data_1178_all_knn, select = -c(ad_id,xyz_campaign_id))

split <- sample.split(data_1178_knn , SplitRatio = 0.7)
data_1178_knn_m <- sapply(data_1178_knn, as.numeric)
data_1178_knn <- as.data.frame(data_1178_knn_m)
train_knn <- subset(data_1178_knn, split == "TRUE")
test_knn <- subset(data_1178_knn, split == "FALSE")
# Feature Scaling

  
# Fitting KNN Model 
# to training dataset
classifier_knn <- knn(train = train_knn,
                      test = test_knn,
                      cl = train_knn$target,
                      k = 4)

# Confusion Matrix
con_matrix_knn <- table(test_knn$target, classifier_knn)
con_matrix_knn
pred_test <- as.data.frame(classifier_knn)[,1]
# Model Evaluation - Choosing K
# Calculate out of Sample error
misClassError <- mean(classifier_knn != test_knn$target)
print(paste('Accuracy =', 1-misClassError))

class(pred_test)
knn_roc <- roc(test_knn$target, as.numeric(pred_test))
knn_auc <- knn_roc$auc
knn_auc
auc_label_knn <- paste("AUC = ", knn_auc)
roc_plot <- ggroc(knn_roc, legacy.axes = TRUE)+
  labs(title = "ROC Curve of KNN ", subtitle = auc_label_knn)+
  geom_area(aes(x = 1 - specificity, y = sensitivity),fill = "blue", alpha = 0.2)
roc_plot
```




##4.1 neural network
```{r}
library(neuralnet)
set.seed(12)
data_clean_nn <- data %>% 
  mutate(age = as.factor(age),
         gender = as.factor(gender),
         interest = as.factor(interest),
         Spent = scale(Spent),
         Impressions = scale(Impressions),
         Clicks = scale(Clicks),
         target = ifelse(Approved_Conversion==0,0,1)
  )
data_clean_nn1<-na.omit(data_clean_nn)
data_clean_nn <- subset(data_clean_nn1, select= -c(fb_campaign_id,Approved_Conversion))

data_1178_all_nn <- data_clean_nn %>% filter(xyz_campaign_id==1178)
data_1178_nn <- subset(data_1178_all_nn, select = -c(ad_id,xyz_campaign_id))
ind2 <- 1:(nrow(data_1178_nn)/4)
train_data_nn_raw <- data_1178_nn[-ind1, ]
train_data_nn <- train_data_nn_raw %>% mutate_all(as.numeric)
test_data_nn_raw <- data_1178_nn[ind1, ]
    test_data_nn <- test_data_nn_raw %>% mutate_all(as.numeric)

nn <- neuralnet(factor(target) ~ ., train_data_nn, hidden = c(3,2))
plot (nn)
# Make predictions on the test data
net_prediction<-predict (nn, test_data_nn) 
#confusion matrix
target1 <- factor(test_data_nn$target, levels = c(0, 1))
net_prediction1 <- factor(ifelse(net_prediction[,1] > net_prediction[,2], 0, 1), levels=c(0, 1))
confusionmatrix_nn <-confusionMatrix(target1,net_prediction1)
confusionmatrix_nn
# convert net_prediction to a numeric vector
net_pred_numeric <- as.numeric(net_prediction[,1])

# convert net_pred_numeric to a factor with the same levels as test_data_nn$target
net_pred_factor <- factor(ifelse(net_pred_numeric > 0.5, 0, 1))
# convert net_prediction to a numeric vector
net_pred_numeric <- as.numeric(net_pred_factor)
nn_roc <- roc(test_data_nn$target, net_pred_numeric)
nn_auc <- nn_roc$auc
nn_auc
roc_plot_nn <- ggroc(nn_roc, legacy.axes = TRUE)
# add the AUC value to the plot
auc_label_nn <- paste("AUC = ", nn_auc)
roc_plot_nn <- roc_plot_nn +
  labs(title = "ROC Curve", subtitle = auc_label_nn)+
  geom_area(aes(x = 1 - specificity, y = sensitivity),
                                 fill = "blue", alpha = 0.2)
roc_plot_nn
```


##5.1 random forest
```{r}
set.seed(12)
#data_1178 <- data %>% filter(xyz_campaign_id==1178) # filter one company campaign 
#data_1178 <- data_1178 %>% mutate(target=ifelse(Approved_Conversion==0,0,1)) # add a new viariable
data_1178 <- data_1178_nn
data_1178 <- cbind(data_1178,as.data.frame(model.matrix(~ gender-1, data_1178))) # add dummy gender variable
data_1178$gender <- NULL # remove original gender variable
data_1178 <- cbind(data_1178,as.data.frame(model.matrix(~ age-1, data_1178))) # add dummy age variable  
data_1178$age <- NULL # remove original age variable
#data_1178$random <- runif(625) # add random variable if necessary
split_rf <- initial_split(data_1178)
train_data_rf <- training(split_rf)
test_data_rf <- testing(split_rf) # split training and test
train_ctrl <- trainControl(method = "cv",number = 5) 
# train rf to measure conversion probability
fit_rf <- train(
  factor(target) ~ Spent + factor(interest) + genderM + `age30-34` + `age35-39` + `age40-44` + `age45-49`, 
  method = "rf", 
  data = train_data_rf,
  trControl = train_ctrl,
  ntree = 20,
  importance = TRUE
)

# cross-tabulation of observed and predicted classes
p_rf <- predict(fit_rf, type="raw", newdata = test_data_rf)
tab <- table(Actual=test_data_rf$target, Reference=p_rf)
confusionMatrix(tab)

# add conversion probability variable
test_data_rf <- cbind(test_data_rf,p_rf)
colnames(test_data_rf)[13] <- "predict"

imp <- varImp(fit_rf)

options(repr.plot.width=15, repr.plot.height=15)

ggplot(
  imp, aes(names,1)) + 
  geom_point() +
  ggthemes::theme_fivethirtyeight() +
  theme(
    plot.title = element_text(size=16), 
    plot.subtitle = element_text(size=8), 
    axis.text=element_text(size=8), 
    legend.position = "none", 
    panel.grid.major = element_line(size = 0.7, linetype="dotted"), 
    panel.grid.major.x = element_blank()
    ) +
  labs(title = 'Random Forest Model Variable Importance')

# Create ROC curve
roc_curve <- roc(test_data_rf$target, as.numeric(test_data_rf$predict))
nf_auc <- roc_curve$auc
nf_auc
# Create AUC plot
roc_plot_rf <- ggroc(roc_curve, legacy.axes = TRUE)
# add the AUC value to the plot
auc_label_rf <- paste("AUC = ", nf_auc)
roc_plot_rf <- roc_plot_rf + labs(title = "ROC Curve", subtitle = auc_label_rf)
roc_plot_rf <- roc_plot_rf + geom_area(aes(x = 1 - specificity, y = sensitivity),
                                 fill = "blue", alpha = 0.2)
roc_plot_rf
```
Random forest will be excluded here since its AUC value is only 0.6, which is far from other models' results. 
##6.1 gbm
```{r}
library(gbm)
set.seed(12)
data_1178_all_gbm <- data_clean %>% filter(xyz_campaign_id==1178)
data_1178_gbm <- subset(data_1178_all_gbm, select = -c(ad_id,xyz_campaign_id))%>% 
  mutate_all(as.numeric)
data_1178_gbm$target <- as.numeric(data_1178_gbm$target)-1
split_gbm <- initial_split(data_1178_gbm)
train_data_gbm <- training(split_gbm)
test_data_gbm <- testing(split_gbm)

# Fit the gradient boosting model
gbm_fit <- gbm(target ~ ., train_data_gbm, distribution = 'bernoulli', n.trees = 20,cv.folds = 5,interaction.depth = 1, shrinkage = 0.2)
best_n_trees <- which.min(gbm_fit$cv.error)
best_n_trees
summary(gbm_fit)
# Make predictions on test set
gbm_pred <- predict.gbm(gbm_fit, test_data_gbm, n.trees = best_n_trees, type = "response")
# Create ROC curve
roc_obj <- roc(test_data_gbm$target, gbm_pred)
roc_plot_gbm <- ggroc(roc_obj, legacy.axes = TRUE)
gbm_auc <- roc_obj$auc
auc_label_gbm <- paste("AUC = ", gbm_auc)
roc_plot_gbm <- roc_plot_gbm + labs(title = "ROC Curve", subtitle = auc_label_gbm)
roc_plot_gbm <- roc_plot_gbm + geom_area(aes(x = 1 - specificity, y = sensitivity),
                                 fill = "blue", alpha = 0.2)
roc_plot_gbm

# Make predictions on the test set
test_predictions <- predict(gbm_fit, newdata = test_data_gbm, n.trees = best_n_trees, type = "response")
test_predictions <- ifelse(test_predictions > 0.5, 1, 0)
# Convert test_predictions to factor with levels 0 and 1
test_predictions <- factor(test_predictions, levels = c(0, 1))

# Convert test_data_gbm$target to factor with levels 0 and 1
test_data_gbm$target <- factor(test_data_gbm$target, levels = c(0, 1))

# Visualize confusion matrix
library(caret)
confusionMatrix(test_predictions, test_data_gbm$target)

```


##7.1 SVM
```{r}
library(e1071)
set.seed(12)
data_clean_svm <- data_1178_nn
split_svm <- initial_split(data_clean_svm)
train_data_svm <- training(split_svm)
test_data_svm <- testing(split_svm)

svmfit <- svm(target ~ ., data = train_data_svm, cost = 2) 
pred_svm <- predict(svmfit, newdata = test_data_svm, type="class") 

net_pred_svm <- as.matrix(ifelse(pred_svm > 0.5, 1, 0))[,1]
table(net_pred_svm, test_data_svm$target)

# Create ROC curve
roc_svm <- roc(test_data_svm$target, pred_svm)
roc_plot_svm <- ggroc(roc_svm, legacy.axes = TRUE)
svm_auc <- roc_svm$auc
auc_label_svm <- paste("AUC = ", svm_auc)
roc_plot_svm <- roc_plot_svm + labs(title = "ROC Curve of SVM for Train data in campaign 1178", subtitle = auc_label_svm) + geom_area(aes(x = 1 - specificity, y = sensitivity),
                                 fill = "blue", alpha = 0.2)
roc_plot_svm

```

##8.1 Conclusion & pick the best model
```{r}
# Example AUC values
auc_values <- c("Logistic Regression" = 0.8701, "SVM" = 0.7419, "KNN"= 0.8011, "Neural Network" = 0.7097, "Gradient Boosting" = 0.8422)

# Create a table with the AUC values
auc_table <- data.frame(Method = names(auc_values), AUC = auc_values)

# Create a bar plot with the AUC values
auc_plot <- ggplot(auc_table, aes(x = Method, y = AUC)) +
  geom_bar(stat = "identity", fill = "grey") +
  ylim(0, 1) +
  labs(title = "AUC Values for Classification Methods", x = "Classification Method", y = "AUC")

# Print the table and the plot
print(auc_table)
print(auc_plot)

```